# 1.2 集群安装

**本节目标**

*本文描述了如何安装、配置和管理有实际意义的Hadoop集群，其规模可从几个节点的小集群到几千个节点的超大集群。如果你希望在单机上安装Hadoop玩玩，可以参考[单击模式](<01.1.md>)
 
**先决条件**

*下载  Hadoop安装包

**安装**

* 安装Hadoop集群通常要将安装软件解压到集群内的所有机器上。
* 通常一台机器在集群里呗被指定微NameNode,另外一台机器作为ResourceManager，这两太作为Masters，剩下其他的机器在集群中作为Datanode和nodeManger，这些机器是salves。

**在非安全模式下运行Hadoop**

* 下面步骤描述如何配置Hadoop集群
* 配置文件包括两大类：
	* 一类是只读的Hadoop默认的配置文件：core-default.xml, hdfs-default.xml, yarn-default.xml and mapred-default.xml.。这些配置文件不许要用户更改
	* 另外一类是需要用户配置的配置文件：- conf/core-site.xml, conf/hdfs-site.xml, conf/yarn-site.xml and conf/mapred-site.xml.
* 另外，你也可以配置bin目录下的脚本，并且通过更改conf/hadoop-ev.sh和yarn-evn.sh来改变hadoop的一些配置值。
*  要配置Hadoop集群，你需要设置Hadoop守护进程的运行环境和Hadoop守护进程的运行参数
*  Hadoop守护进程包括Namenode，Datanode，ResourceManager，NodeManager。
* **集群配置**
	* 管理员可在conf/hadoop-env.sh和 conf/yarn-env.sh脚本内对Hadoop守护进程的运行环境做特别指定。
	*  你还需要在每个节点上正确设置好JAVA_HOME。
	*  管理员可以通过配置选项HADOOP_*_OPTS来分别配置各个守护进程。 下表是可以配置的选项。
<table>
		<tr><td>守护进程</td><td>配置选项</td></tr>
		<tr><td>NameNode</td><td>HADOOP_NAMENODE_OPTS</td></tr>
		<tr><td>DataNode</td><td>HADOOP_DATANODE_OPTS</td></tr>
		<tr><td>SecondaryNamenode</td><td>HADOOP_SECONDARYNAMENODE_OPTS</td></tr>
		<tr><td>JobTracker</td><td>HADOOP_JOBTRACKER_OPTS</td></tr>
		<tr><td>TaskTracker</td><td>HADOOP_TASKTRACKER_OPTS</td></tr>
</table>
* 例如，配置Namenode时,为了使其能够并行回收垃圾（parallelGC）， 要把下面的代码加入到hadoop-env.sh : 
export HADOOP_NAMENODE_OPTS="-XX:+UseParallelGC ${HADOOP_NAMENODE_OPTS}" 
* 其他常用的配置参数还包括：
	*  HADOOP_LOG_DIR/YARN_LOG.守护进程日志配置目录，如果这个文件不存在，应用程序会自动创建。
	*  HADOOP_HEAPSIZE/YARN_HEAPSIZE.最大可用的堆大小，单位为MB。比如，1000MB。 这个参数用于设置hadoop守护进程的堆大小。缺省大小是1000MB,配置选项如下：
<table>
		<tr><td>守护进程</td><td>配置选项</td></tr>
		<tr><td>ResourceManager</td><td>YARN_RESOURCEMANAGER_HEAPSIZE</td></tr>
		<tr><td>NodeManager</td><td>YARN_NODEMANAGER_HEAPSIZE</td></tr>
		<tr><td>WebAppProxy</td><td>YARN_PROXYSERVER_HEAPSIZE</td></tr>
		<tr><td>Map Reduce Job History Server</td><td>HADOOP_JOB_HISTORYSERVER_HEAPSIZE</td></tr>
</table>
* 使用 Non-Secure模式配置Hadoop守护进程
	* 本小结描述了配置文件一些重要的参数：
		* conf/core-site.xml
			<table><tr><td>参数</td><td>值</td><td>备注</td></tr><tr><td>fs.defaultFS</td><td>NameNode URI	</td><td>hdfs://host:port/</td></tr><tr><td>io.file.buffer.size</td><td>131072</td><td>Size of read/write buffer used in SequenceFiles.</td></tr></table>
		* conf/hdfs-site.xml
			* Namenode配置选项：
				<table><td>参数</td><td>值</td><td>备注</td><tr><td>fs.namenode.name.dir</td>	<td>Path on the local filesystem where the NameNode stores the namespace and transactions logs persistently.</td><td>If this is a comma-delimited list of directories then the name table is replicated in all of the directories, for redundancy.</td></tr><tr>
<td>dfs.namenode.hosts / dfs.namenode.hosts.exclude</td><td>List of permitted/excluded DataNodes.</td><td>If necessary, use these files to control the list of allowable datanodes.</td></tr>
<tr><td>dfs.blocksize</td><td>268435456</td><td>	HDFS blocksize of 256MB for large file-systems.</td></tr>
<tr><td>dfs.namenode.handler.count</td><td>100</td><td>	More NameNode server threads to handle RPCs from large number of DataNodes.</td></tr></table>
			* Datanode配置选项：
				<table><td>参数</td><td>值</td><td>备注</td><tr><td>dfs.datanode.data.dir</td>	<td>Comma separated list of paths on the local filesystem of a DataNode where it should store its blocks.</td><td>If this is a comma-delimited list of directories, then data will be stored in all named directories, typically on different devices</td></tr></table>
		* conf/yarn-site.xml
			*  ResourceManager和NodeManager配置选项：
				<table><td>参数</td><td>值</td><td>备注</td><tr><td>dfs.datanode.data.dir</td>	<td>Comma separated list of paths on the local filesystem of a DataNode where it should store its blocks.</td><td>If this is a comma-delimited list of directories, then data will be stored in all named directories, typically on different devices</td></tr></table>
		 * ResourceManager配置选项:<table border="1" class="bodyTable"><tr class="a"><th align="left">参数</th><th align="left">值</th><th align="left">备注</th></tr><tr class="b"><td align="left"><tt>yarn.resourcemanager.address</tt></td><td align="left"><tt>ResourceManager</tt> host:port for clients to submit jobs.</td><td align="left">_host:port_</td></tr><tr class="a"><td align="left"><tt>yarn.resourcemanager.scheduler.address</tt></td><td align="left"><tt>ResourceManager</tt> host:port for ApplicationMasters to talk to Scheduler to obtain resources.</td><td align="left">_host:port_</td></tr><tr class="b"><td align="left"><tt>yarn.resourcemanager.resource-tracker.address</tt></td><td align="left"><tt>ResourceManager</tt> host:port for NodeManagers.</td><td align="left">_host:port_</td></tr><tr class="a"><td align="left"><tt>yarn.resourcemanager.admin.address</tt></td><td align="left"><tt>ResourceManager</tt> host:port for administrative commands.</td><td align="left">_host:port_</td></tr><tr class="b"><td align="left"><tt>yarn.resourcemanager.webapp.address</tt></td><td align="left"><tt>ResourceManager</tt> web-ui host:port.</td><td align="left">_host:port_</td></tr><tr class="a"><td align="left"><tt>yarn.resourcemanager.scheduler.class</tt></td><td align="left"><tt>ResourceManager</tt> Scheduler class.</td><td align="left"><tt>CapacityScheduler</tt> (recommended), <tt>FairScheduler</tt> (also recommended), or <tt>FifoScheduler</tt></td></tr><tr class="b"><td align="left"><tt>yarn.scheduler.minimum-allocation-mb</tt></td><td align="left">Minimum limit of memory to allocate to each container request at the <tt>Resource Manager</tt>.</td><td align="left">In MBs</td></tr><tr class="a"><td align="left"><tt>yarn.scheduler.maximum-allocation-mb</tt></td><td align="left">Maximum limit of memory to allocate to each container request at the <tt>Resource Manager</tt>.</td><td align="left">In MBs</td></tr><tr class="b"><td align="left"><tt>yarn.resourcemanager.nodes.include-path</tt> / <tt>yarn.resourcemanager.nodes.exclude-path</tt></td><td align="left">List of permitted/excluded NodeManagers.</td><td align="left">If necessary, use these files to control the list of allowable NodeManagers.</td></tr></table>
	    * NodeManager配置选项:<table border="1" class="bodyTable"><tr class="a"><th align="left">参数</th><th align="left">值</th><th align="left">备注</th></tr><tr class="b"><td align="left"><tt>yarn.nodemanager.resource.memory-mb</tt></td><td align="left">Resource i.e. available physical memory, in MB, for given <tt>NodeManager</tt></td><td align="left">Defines total available resources on the <tt>NodeManager</tt> to be made available to running containers</td></tr><tr class="a"><td align="left"><tt>yarn.nodemanager.vmem-pmem-ratio</tt></td><td align="left">Maximum ratio by which virtual memory usage of tasks may exceed physical memory</td><td align="left">The virtual memory usage of each task may exceed its physical memory limit by this ratio. The total amount of virtual memory used by tasks on the NodeManager may exceed its physical memory usage by this ratio.</td></tr><tr class="b"><td align="left"><tt>yarn.nodemanager.local-dirs</tt></td><td align="left">Comma-separated list of paths on the local filesystem where intermediate data is written.</td><td align="left">Multiple paths help spread disk i/o.</td></tr><tr class="a"><td align="left"><tt>yarn.nodemanager.log-dirs</tt></td><td align="left">Comma-separated list of paths on the local filesystem where logs are written.</td><td align="left">Multiple paths help spread disk i/o.</td></tr><tr class="b"><td align="left"><tt>yarn.nodemanager.log.retain-seconds</tt></td><td align="left">_10800_</td><td align="left">Default time (in seconds) to retain log files on the NodeManager Only applicable if log-aggregation is disabled.</td></tr><tr class="a"><td align="left"><tt>yarn.nodemanager.remote-app-log-dir</tt></td><td align="left">_/logs_</td><td align="left">HDFS directory where the application logs are moved on application completion. Need to set appropriate permissions. Only applicable if log-aggregation is enabled.</td></tr><tr class="b"><td align="left"><tt>yarn.nodemanager.remote-app-log-dir-suffix</tt></td><td align="left">_logs_</td><td align="left">Suffix appended to the remote log dir. Logs will be aggregated to ${yarn.nodemanager.remote-app-log-dir}/${user}/${thisParam} Only applicable if log-aggregation is enabled.</td></tr><tr class="a"><td align="left"><tt>yarn.nodemanager.aux-services</tt></td><td align="left">mapreduce_shuffle</td><td align="left">Shuffle service that needs to be set for Map Reduce applications.</td></tr></table>
    *   History Server配置选项 (Needs to be moved elsewhere):<table border="1" class="bodyTable"><tr class="a"><th align="left">参数</th><th align="left">值</th><th align="left">备注</th></tr><tr class="b"><td align="left"><tt>yarn.log-aggregation.retain-seconds</tt></td><td align="left">_-1_</td><td align="left">How long to keep aggregation logs before deleting them. -1 disables. Be careful, set this too small and you will spam the name node.</td></tr><tr class="a"><td align="left"><tt>yarn.log-aggregation.retain-check-interval-seconds</tt></td><td align="left">_-1_</td><td align="left">Time between checks for aggregated log retention. If set to 0 or a negative value then the value is computed as one-tenth of the aggregated log retention time. Be careful, set this too small and you will spam the name node.</td></tr></table>
*   <tt>conf/mapred-site.xml</tt>
	* MapReduce配置选项:
	        <table border="1" class="bodyTable"><tr class="a"><th align="left">参数</th><th align="left">值</th><th align="left">备注</th></tr><tr class="b"><td align="left"><tt>mapreduce.framework.name</tt></td><td align="left">yarn</td><td align="left">Execution framework set to Hadoop YARN.</td></tr><tr class="a"><td align="left"><tt>mapreduce.map.memory.mb</tt></td><td align="left">1536</td><td align="left">Larger resource limit for maps.</td></tr><tr class="b"><td align="left"><tt>mapreduce.map.java.opts</tt></td><td align="left">-Xmx1024M</td><td align="left">Larger heap-size for child jvms of maps.</td></tr><tr class="a"><td align="left"><tt>mapreduce.reduce.memory.mb</tt></td><td align="left">3072</td><td align="left">Larger resource limit for reduces.</td></tr><tr class="b"><td align="left"><tt>mapreduce.reduce.java.opts</tt></td><td align="left">-Xmx2560M</td><td align="left">Larger heap-size for child jvms of reduces.</td></tr><tr class="a"><td align="left"><tt>mapreduce.task.io.sort.mb</tt></td><td align="left">512</td><td align="left">Higher memory-limit while sorting data for efficiency.</td></tr><tr class="b"><td align="left"><tt>mapreduce.task.io.sort.factor</tt></td><td align="left">100</td><td align="left">More streams merged at once while sorting files.</td></tr><tr class="a"><td align="left"><tt>mapreduce.reduce.shuffle.parallelcopies</tt></td><td align="left">50</td><td align="left">Higher number of parallel copies run by reduces to fetch outputs from very large number of maps.</td></tr></table>
    * MapReduce JobHistory Server配置选项:<table border="1" class="bodyTable"><tr class="a"><th align="left">参数</th><th align="left">值</th><th align="left">备注</th></tr><tr class="b"><td align="left"><tt>mapreduce.jobhistory.address</tt></td><td align="left">MapReduce JobHistory Server _host:port_</td><td align="left">Default port is 10020.</td></tr><tr class="a"><td align="left"><tt>mapreduce.jobhistory.webapp.address</tt></td><td align="left">MapReduce JobHistory Server Web UI _host:port_</td><td align="left">Default port is 19888.</td></tr><tr class="b"><td align="left"><tt>mapreduce.jobhistory.intermediate-done-dir</tt></td><td align="left">/mr-history/tmp</td><td align="left">Directory where history files are written by MapReduce jobs.</td></tr><tr class="a"><td align="left"><tt>mapreduce.jobhistory.done-dir</tt></td><td align="left">/mr-history/done</td><td align="left">Directory where history files are managed by the MR JobHistory Server.</td></tr></table></div></div><div class="section">

### <a name="Hadoop_Rack_Awareness">Hadoop的机架感知</a>

HDFS和YARN具备机架感知策略.

NameNode和ResourceManager通过调用管理员配置模块中的API resolve来获取集群里每个slave的机架id

API将dns域名或者ip转换微salves的机架id

通过配置项<tt>topology.node.switch.mapping.impl</tt>.来制定特定的使用模块，模块的默认实现会调用<tt>topology.script.file.name</tt>配置项指定的一个的脚本/命令. If <tt>topology.script.file.name</tt>  如果<tt》topology.script.file.name</tt>未被设置，对于所有传入的IP地址，模块会返回/default-rack作为机架id.
</div><div class="section">

### <a name="Monitoring_Health_of_NodeManagers">NodeManager的健康监控</a>

Hadoop provides a mechanism by which administrators can configure the NodeManager to run an administrator supplied script periodically to determine if a node is healthy or not.

Administrators can determine if the node is in a healthy state by performing any checks of their choice in the script. If the script detects the node to be in an unhealthy state, it must print a line to standard output beginning with the string ERROR. The NodeManager spawns the script periodically and checks its output. If the script's output contains the string ERROR, as described above, the node's status is reported as <tt>unhealthy</tt> and the node is black-listed by the ResourceManager. No further tasks will be assigned to this node. However, the NodeManager continues to run the script, so that if the node becomes healthy again, it will be removed from the blacklisted nodes on the ResourceManager automatically. The node's health along with the output of the script, if it is unhealthy, is available to the administrator in the ResourceManager web interface. The time since the node was healthy is also displayed on the web interface.

The following parameters can be used to control the node health monitoring script in <tt>conf/yarn-site.xml</tt>.
<table border="1" class="bodyTable"><tr class="a"><th align="left">Parameter</th><th align="left">Value</th><th align="left">Notes</th></tr><tr class="b"><td align="left"><tt>yarn.nodemanager.health-checker.script.path</tt></td><td align="left">Node health script</td><td align="left">Script to check for node's health status.</td></tr><tr class="a"><td align="left"><tt>yarn.nodemanager.health-checker.script.opts</tt></td><td align="left">Node health script options</td><td align="left">Options for script to check for node's health status.</td></tr><tr class="b"><td align="left"><tt>yarn.nodemanager.health-checker.script.interval-ms</tt></td><td align="left">Node health script interval</td><td align="left">Time interval for running health script.</td></tr><tr class="a"><td align="left"><tt>yarn.nodemanager.health-checker.script.timeout-ms</tt></td><td align="left">Node health script timeout interval</td><td align="left">Timeout for health script execution.</td></tr></table>

The health checker script is not supposed to give ERROR if only some of the local disks become bad. NodeManager has the ability to periodically check the health of the local disks (specifically checks nodemanager-local-dirs and nodemanager-log-dirs) and after reaching the threshold of number of bad directories based on the value set for the config property yarn.nodemanager.disk-health-checker.min-healthy-disks, the whole node is marked unhealthy and this info is sent to resource manager also. The boot disk is either raided or a failure in the boot disk is identified by the health checker script.
</div><div class="section">

### <a name="Slaves_file">Slaves file</a>

Typically you choose one machine in the cluster to act as the NameNode and one machine as to act as the ResourceManager, exclusively. The rest of the machines act as both a DataNode and NodeManager and are referred to as _slaves_.

List all slave hostnames or IP addresses in your <tt>conf/slaves</tt> file, one per line.
</div><div class="section">

### <a name="Logging">Logging</a>

Hadoop uses the Apache log4j via the Apache Commons Logging framework for logging. Edit the <tt>conf/log4j.properties</tt> file to customize the Hadoop daemons' logging configuration (log-formats and so on).
</div><div class="section">

### <a name="Operating_the_Hadoop_Cluster">Operating the Hadoop Cluster</a>

Once all the necessary configuration is complete, distribute the files to the <tt>HADOOP_CONF_DIR</tt> directory on all the machines.
<div class="section">

#### Hadoop Startup<a name="Hadoop_Startup"></a>

To start a Hadoop cluster you will need to start both the HDFS and YARN cluster.

Format a new distributed filesystem:
<div><pre>$ $HADOOP_PREFIX/bin/hdfs namenode -format &lt;cluster_name&gt;</pre></div>

Start the HDFS with the following command, run on the designated NameNode:
<div><pre>$ $HADOOP_PREFIX/sbin/hadoop-daemon.sh --config $HADOOP_CONF_DIR --script hdfs start namenode</pre></div>

Run a script to start DataNodes on all slaves:
<div><pre>$ $HADOOP_PREFIX/sbin/hadoop-daemon.sh --config $HADOOP_CONF_DIR --script hdfs start datanode</pre></div>

Start the YARN with the following command, run on the designated ResourceManager:
<div><pre>$ $HADOOP_YARN_HOME/sbin/yarn-daemon.sh --config $HADOOP_CONF_DIR start resourcemanager</pre></div>

Run a script to start NodeManagers on all slaves:
<div><pre>$ $HADOOP_YARN_HOME/sbin/yarn-daemon.sh --config $HADOOP_CONF_DIR start nodemanager</pre></div>

Start a standalone WebAppProxy server. If multiple servers are used with load balancing it should be run on each of them:
<div><pre>$ $HADOOP_YARN_HOME/sbin/yarn-daemon.sh start proxyserver --config $HADOOP_CONF_DIR</pre></div>

Start the MapReduce JobHistory Server with the following command, run on the designated server:
<div><pre>$ $HADOOP_PREFIX/sbin/mr-jobhistory-daemon.sh start historyserver --config $HADOOP_CONF_DIR</pre></div></div><div class="section">

#### Hadoop Shutdown<a name="Hadoop_Shutdown"></a>

Stop the NameNode with the following command, run on the designated NameNode:
<div><pre>$ $HADOOP_PREFIX/sbin/hadoop-daemon.sh --config $HADOOP_CONF_DIR --script hdfs stop namenode</pre></div>

Run a script to stop DataNodes on all slaves:
<div><pre>$ $HADOOP_PREFIX/sbin/hadoop-daemon.sh --config $HADOOP_CONF_DIR --script hdfs stop datanode</pre></div>

Stop the ResourceManager with the following command, run on the designated ResourceManager:
<div><pre>$ $HADOOP_YARN_HOME/sbin/yarn-daemon.sh --config $HADOOP_CONF_DIR stop resourcemanager</pre></div>

Run a script to stop NodeManagers on all slaves:
<div><pre>$ $HADOOP_YARN_HOME/sbin/yarn-daemon.sh --config $HADOOP_CONF_DIR stop nodemanager</pre></div>

Stop the WebAppProxy server. If multiple servers are used with load balancing it should be run on each of them:
<div><pre>$ $HADOOP_YARN_HOME/sbin/yarn-daemon.sh stop proxyserver --config $HADOOP_CONF_DIR</pre></div>

Stop the MapReduce JobHistory Server with the following command, run on the designated server:
<div><pre>$ $HADOOP_PREFIX/sbin/mr-jobhistory-daemon.sh stop historyserver --config $HADOOP_CONF_DIR</pre></div></div></div><div class="section">
 

----
##![](images/hadoop-logo.jpg?raw=true)
  * 上一节: [单击模式安装](<01.2.md>)
  * 下一节: [Hadoop命令手册](<01.4.md>)
